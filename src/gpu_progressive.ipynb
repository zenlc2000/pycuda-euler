{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive read size evaluation\n",
    "\n",
    "### This section is using the big 15GB data file as a source. See below for new configuration.\n",
    "We know we can do 10k reads so let's take progressively larger number of reads to show a progression of processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import eulercuda as ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycuda.driver\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "In this case I'm going to read a local data file into a list then ranomize the list and take slices. Then use Spark to parallelize the slice to turn it into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = '/home/ubuntu/genome/Ecoli-RR359304-2.fastq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_fastq(filename, total):\n",
    "    \"\"\"\n",
    "    Read fastq formatted <filename> and return a list of reads\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    infile = open(filename, \"r\")\n",
    "    for i,line in tqdm(enumerate(infile), desc='parsing', total=total):\n",
    "        if i % 4 == 1:\n",
    "            result.append(line.rstrip())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = 389607888 # hardcoded b/c this won't change for this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing: 100%|██████████| 389607888/389607888 [05:43<00:00, 1135714.40it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_data = read_fastq('/home/ubuntu/genome/Ecoli-RR359304-2.fastq',lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import choice, shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle(raw_data) # maybe shuffle isn't needed with choice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(num_reads, raw_data):\n",
    "    data = []\n",
    "    seq = ''\n",
    "    for i in range(num_reads):\n",
    "        seq = choice(raw_data)\n",
    "        while 'N' in seq:\n",
    "            seq = choice(raw_data)\n",
    "        data.append(choice(raw_data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10K reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = build_dataset(10000, raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_data = sc.parallelize(data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataLength = len(data[0])\n",
    "dataCount = rdd_data.count() // rdd_data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 17\n",
    "lmerLength = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hail_mary(path, data):\n",
    "    import eulercuda.eulercuda as ec\n",
    "    subprocess.call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", \"-f\", '/genome/' + path])\n",
    "    hail_mary = data.mapPartitions(lambda x: ec.assemble2(k, buffer=x, readLength = dataLength,readCount=dataCount)) \\\n",
    "        .saveAsTextFile('hdfs://172.31.26.32/genome/' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 22.9 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hail_mary('ec10k_output',rdd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 15K reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = build_dataset(15000,raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_data = sc.parallelize(data, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataLength = len(data[0])\n",
    "dataCount = rdd_data.count() // rdd_data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2142"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 17\n",
    "lmerLength = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 32 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hail_mary('ec15k_output',rdd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20K reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = build_dataset(20000,raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_data = sc.parallelize(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataLength = len(data[0])\n",
    "dataCount = rdd_data.count() // rdd_data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 37.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hail_mary('ec20k_output',rdd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25K reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = build_dataset(25000,raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_data = sc.parallelize(data, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2083"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataLength = len(data[0])\n",
    "dataCount = rdd_data.count() // rdd_data.getNumPartitions()\n",
    "dataCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 46.1 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hail_mary('ec25k_output',rdd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30K reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = build_dataset(30000,raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_data = sc.parallelize(data, (data_size // 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataLength = len(data[0])\n",
    "dataCount = rdd_data.count() // rdd_data.getNumPartitions()\n",
    "dataCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 57 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hail_mary('ec30k_output',rdd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35K reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size = 35000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = build_dataset(data_size, raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_data = sc.parallelize(data, (data_size // 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2058"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataLength = len(data[0])\n",
    "dataCount = rdd_data.count() // rdd_data.getNumPartitions()\n",
    "dataCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1min 5s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hail_mary('ec35k_output',rdd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40K reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size = 35000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = build_dataset(data_size, raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_data = sc.parallelize(data, (data_size // 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2058"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataLength = len(data[0])\n",
    "dataCount = rdd_data.count() // rdd_data.getNumPartitions()\n",
    "dataCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%timeit hail_mary('ec35k_output',rdd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is working well enough maybe we should automate?\n",
    "\n",
    "### This didn't work.  New method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_size = 40000\n",
    "# is_error = False\n",
    "# times = {}\n",
    "# while True:\n",
    "#     print('Starting ' + str(data_size) + '\\n')\n",
    "#     data = build_dataset(data_size, raw_data)\n",
    "#     rdd_data = sc.parallelize(data, (data_size // 2000))\n",
    "#     dataLength = len(data[0])\n",
    "#     dataCount = rdd_data.count() // rdd_data.getNumPartitions()\n",
    "#     print('dataCount = ' + str(dataCount))\n",
    "#     data_time = %timeit -o hail_mary('ec'+str(data_size)[:2]+'k_output',rdd_data)\n",
    "#     print(str(data_size), data_time.best)\n",
    "#     times[data_size]= data_time \n",
    "#     data_size += 5000\n",
    "# #     except:\n",
    "#         is_error = True\n",
    "#         print('Fatal error on ' + str(data_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# New method using \"cleaned\" ecoli data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = '/home/ubuntu/genome/ecoli-clean.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timing_data = {}\n",
    "increment = 5000\n",
    "read_size = 6000\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset(read_size):\n",
    "    dataset = []\n",
    "    with open (datafile, 'r') as infile:\n",
    "        for i in range(read_size):\n",
    "            line = infile.readline().rstrip()\n",
    "            if 'N' not in line:\n",
    "                dataset.append(line)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 6000\n",
      "\n",
      "1 loop, best of 3: 15.8 s per loop\n",
      "6000 15.802363910999702\n",
      "\n",
      "Starting 11000\n",
      "\n",
      "1 loop, best of 3: 24.8 s per loop\n",
      "11000 24.786940824999874\n",
      "\n",
      "Starting 16000\n",
      "\n",
      "1 loop, best of 3: 30.2 s per loop\n",
      "16000 30.179472974999953\n",
      "\n",
      "Starting 21000\n",
      "\n",
      "1 loop, best of 3: 39 s per loop\n",
      "21000 39.00074063300053\n",
      "\n",
      "Starting 26000\n",
      "\n",
      "1 loop, best of 3: 48.9 s per loop\n",
      "26000 48.947659196999666\n",
      "\n",
      "Starting 31000\n",
      "\n",
      "1 loop, best of 3: 58.3 s per loop\n",
      "31000 58.28229980500055\n",
      "\n",
      "Starting 36000\n",
      "\n",
      "1 loop, best of 3: 1min 6s per loop\n",
      "36000 66.2625498310008\n",
      "\n",
      "Starting 41000\n",
      "\n",
      "1 loop, best of 3: 1min 13s per loop\n",
      "41000 73.65370573700056\n",
      "\n",
      "Starting 46000\n",
      "\n",
      "1 loop, best of 3: 1min 22s per loop\n",
      "46000 82.31506324700058\n",
      "\n",
      "Starting 51000\n",
      "\n",
      "1 loop, best of 3: 1min 31s per loop\n",
      "51000 91.65724655099984\n",
      "\n",
      "Starting 56000\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o535.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 4 times, most recent failure: Lost task 0.3 in stage 59.0 (TID 767, ip-172-31-26-33.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1479440346004_0002/container_e57_1479440346004_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1479440346004_0002/container_e57_1479440346004_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/2.4.2.0-258/spark/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/hdp/2.4.2.0-258/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"<ipython-input-14-ac16707478c8>\", line 4, in <lambda>\n  File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 500, in assemble2\n    lmerLength, evList, eeList, levEdgeList, entEdgeList, readCount)\n  File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 262, in constructDebruijnGraph\n    d_TK, d_TV, d_bucketSize, bucketCount, d_ev, d_levEdge, d_entEdge, d_ee, readLength)\n  File \"./eulercuda.zip/eulercuda/pydebruijn.py\", line 618, in construct_debruijn_graph_device\n    bucketCount, d_l, d_e, d_ee, d_lstart, d_estart, valid_bitmask)\n  File \"./eulercuda.zip/eulercuda/pydebruijn.py\", line 506, in setup_edges_device\n    np_d_l.get(d_l)\n  File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/gpuarray.py\", line 272, in get\n    _memcpy_discontig(ary, self, async=async, stream=stream)\n  File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/gpuarray.py\", line 1191, in _memcpy_discontig\n    drv.memcpy_dtoh(dst, src.gpudata)\npycuda._driver.LogicError: cuMemcpyDtoH failed: an illegal memory access was encountered\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1855)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1945)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1464)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1443)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1479440346004_0002/container_e57_1479440346004_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1479440346004_0002/container_e57_1479440346004_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/2.4.2.0-258/spark/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/hdp/2.4.2.0-258/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"<ipython-input-14-ac16707478c8>\", line 4, in <lambda>\n  File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 500, in assemble2\n    lmerLength, evList, eeList, levEdgeList, entEdgeList, readCount)\n  File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 262, in constructDebruijnGraph\n    d_TK, d_TV, d_bucketSize, bucketCount, d_ev, d_levEdge, d_entEdge, d_ee, readLength)\n  File \"./eulercuda.zip/eulercuda/pydebruijn.py\", line 618, in construct_debruijn_graph_device\n    bucketCount, d_l, d_e, d_ee, d_lstart, d_estart, valid_bitmask)\n  File \"./eulercuda.zip/eulercuda/pydebruijn.py\", line 506, in setup_edges_device\n    np_d_l.get(d_l)\n  File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/gpuarray.py\", line 272, in get\n    _memcpy_discontig(ary, self, async=async, stream=stream)\n  File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/gpuarray.py\", line 1191, in _memcpy_discontig\n    drv.memcpy_dtoh(dst, src.gpudata)\npycuda._driver.LogicError: cuMemcpyDtoH failed: an illegal memory access was encountered\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-25ed6aefd301>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mrdd_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mread_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdataCount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mrdd_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"timeit -o hail_mary('ec'+str(read_size)[:2]+'k_output',rdd_data)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtiming_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mread_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m                 \u001b[0mtime_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mworst_tuning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworst_tuning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-ac16707478c8>\u001b[0m in \u001b[0;36mhail_mary\u001b[1;34m(path, data)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0meulercuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meulercuda\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"hdfs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dfs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-rm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/genome/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mhail_mary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massemble2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreadLength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataLength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreadCount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hdfs://172.31.26.32/genome/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/hdp/2.4.2.0-258/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1504\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1506\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.4.2.0-258/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.4.2.0-258/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.4.2.0-258/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o535.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 4 times, most recent failure: Lost task 0.3 in stage 59.0 (TID 767, ip-172-31-26-33.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1479440346004_0002/container_e57_1479440346004_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1479440346004_0002/container_e57_1479440346004_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/2.4.2.0-258/spark/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/hdp/2.4.2.0-258/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"<ipython-input-14-ac16707478c8>\", line 4, in <lambda>\n  File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 500, in assemble2\n    lmerLength, evList, eeList, levEdgeList, entEdgeList, readCount)\n  File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 262, in constructDebruijnGraph\n    d_TK, d_TV, d_bucketSize, bucketCount, d_ev, d_levEdge, d_entEdge, d_ee, readLength)\n  File \"./eulercuda.zip/eulercuda/pydebruijn.py\", line 618, in construct_debruijn_graph_device\n    bucketCount, d_l, d_e, d_ee, d_lstart, d_estart, valid_bitmask)\n  File \"./eulercuda.zip/eulercuda/pydebruijn.py\", line 506, in setup_edges_device\n    np_d_l.get(d_l)\n  File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/gpuarray.py\", line 272, in get\n    _memcpy_discontig(ary, self, async=async, stream=stream)\n  File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/gpuarray.py\", line 1191, in _memcpy_discontig\n    drv.memcpy_dtoh(dst, src.gpudata)\npycuda._driver.LogicError: cuMemcpyDtoH failed: an illegal memory access was encountered\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1855)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1945)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1464)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1443)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1479440346004_0002/container_e57_1479440346004_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1479440346004_0002/container_e57_1479440346004_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/2.4.2.0-258/spark/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/hdp/2.4.2.0-258/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"<ipython-input-14-ac16707478c8>\", line 4, in <lambda>\n  File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 500, in assemble2\n    lmerLength, evList, eeList, levEdgeList, entEdgeList, readCount)\n  File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 262, in constructDebruijnGraph\n    d_TK, d_TV, d_bucketSize, bucketCount, d_ev, d_levEdge, d_entEdge, d_ee, readLength)\n  File \"./eulercuda.zip/eulercuda/pydebruijn.py\", line 618, in construct_debruijn_graph_device\n    bucketCount, d_l, d_e, d_ee, d_lstart, d_estart, valid_bitmask)\n  File \"./eulercuda.zip/eulercuda/pydebruijn.py\", line 506, in setup_edges_device\n    np_d_l.get(d_l)\n  File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/gpuarray.py\", line 272, in get\n    _memcpy_discontig(ary, self, async=async, stream=stream)\n  File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/gpuarray.py\", line 1191, in _memcpy_discontig\n    drv.memcpy_dtoh(dst, src.gpudata)\npycuda._driver.LogicError: cuMemcpyDtoH failed: an illegal memory access was encountered\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "k = 17\n",
    "lmerLength = 18\n",
    "while True:\n",
    "    print('Starting ' + str(read_size) + '\\n')\n",
    "    dataset = make_dataset(read_size)\n",
    "    dataLength = len(dataset[0])\n",
    "    rdd_data = sc.parallelize(dataset, (read_size // 2000))\n",
    "    dataCount = rdd_data.count() // rdd_data.getNumPartitions()\n",
    "    time = %timeit -o hail_mary('ec'+str(read_size)[:2]+'k_output',rdd_data)\n",
    "    timing_data[read_size] = time\n",
    "    print(read_size,time.best)\n",
    "    print()\n",
    "    read_size += increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_precision',\n",
       " '_repr_pretty_',\n",
       " 'all_runs',\n",
       " 'best',\n",
       " 'compile_time',\n",
       " 'loops',\n",
       " 'repeat',\n",
       " 'worst']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(timing_data[6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.925483254999563, 15.802363910999702, 15.934597277000648]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing_data[6000].all_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [v.best for v in timing_data.values()]\n",
    "y = [k//1000 for k in timing_data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdd994f8240>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUSKkJK7iEgR+n8T1BjU4mYBwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd99121a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(x, y)\n",
    "# timing_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Spark 1.6.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
