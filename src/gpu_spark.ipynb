{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pycuda-euler \n",
    "\n",
    "GPU assembler using PySpark for distributed computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc.addPyFile(\"eulercuda.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/pycuda-euler/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import eulercuda as ec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycuda.driver\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sc = SparkContext(master='local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and check how many ways Spark/HDFS slices it up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = sc.textFile('hdfs://172.31.26.32:8020/genome/Ba10k.sim1.fq',10)\n",
    "raw_data.getNumPartitions()\n",
    "# raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will either be in \"fasta\" or \"fastq\" format, meaning it will have more than just the DNA letters in it. We'll have to clean that up. \n",
    "\n",
    "**fastq parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = raw_data.filter(lambda x: len(x) > 0 and x[0] in ['A','C','G','T'])\n",
    "dataLength = len(data.take(1)[0])\n",
    "dataCount = data.count() // data.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k value determines how long the dna substrings are. K value is always an odd number and is usually slightly more than 1/2 the size of the read length (read length is number of DNA characters on one line in the data file).\n",
    "\n",
    "It should be noted that although k value is the often-seen metric in the literature, Mahmood's GPU-Euler called out an l (\"ell\") value, because l is one more than k and it is that lth character that forms the edge between two kmer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 17\n",
    "lmerLength = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hail_mary(path, data):\n",
    "    import eulercuda.eulercuda as ec\n",
    "    subprocess.call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", \"-f\", path])\n",
    "    hail_mary = data.mapPartitions(lambda x: ec.assemble2(k, buffer=x, readLength = dataLength,readCount=dataCount)).saveAsTextFile('hdfs://172.31.26.32/genome/gpu_Ba10k_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hail_mary('/genome/gpu_Ba10k_output',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Medium-sized\" 2.2GB file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile('hdfs://172.31.26.32:8020/genome/sra_data.fastq', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = raw_data.filter(lambda x: len(x) > 0 and x[0] in ['A','C','G','T'])\n",
    "dataLength = len(data.take(1)[0])\n",
    "dataCount = data.count() // data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 21\n",
    "lmerLength = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hail_mary = data.mapPartitions(lambda x: ec.assemble2(k, buffer=x, readLength = dataLength,readCount=dataCount)).saveAsTextFile('hdfs://172.31.26.32:8020/genome/gpu_sra_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Cells below are individual functions and notes. Use \"hail_mary\" method above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fasta parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholder cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to think I could simply replace eulercuda.py's assemble2() by replicating function calls here, but it probably won't be that easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readLength = len(data.take(1)[0])\n",
    "readLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readCount = data.count() * readLength\n",
    "readCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original C used a buffer that was one *really* long string. This could be... problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer = data.mapPartitions(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseCount = buffer.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buffer.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glom = buffer.repartition(1)\n",
    "glom.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    evList = []\n",
    "    eeList = []\n",
    "    edgeCount = 0\n",
    "    levEdgeList = []\n",
    "    entEdgeList = []\n",
    "    edgeCount = 0\n",
    "    vertexCount = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "**Important distinction:** \n",
    "if we have a one-argument function:\n",
    "\n",
    "    def myFunc(x):\n",
    "\n",
    "we can call it like this:\n",
    "\n",
    "    myNewRDD = rdd.map(myFunc)\n",
    "\n",
    "and rdd will be passed to myFunc as parameter x. **But** if we have multiple parameters, we handle it with lambda:\n",
    "\n",
    "    def myOtherFunc(x, y, z):\n",
    "    \n",
    "    newRDD = rdd.map(lambda j: myOtherFunc(j, y, z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use lambda form above. Each buffer partition will be passed to constructDebruijnGraph\n",
    "partitionReadCount = readCount // buffer.getNumPartitions()\n",
    "rdd_graph = glom.mapPartitions(lambda x: ec.constructDebruijnGraph(x, partitionReadCount, readLength, lmerLength, evList, eeList, levEdgeList, entEdgeList, readCount))\n",
    "                                \n",
    "et_graph= rdd_graph.mapPartitions(lambda x: ec.findEulerTour(x[1], x[0], x[2], x[3], x[5], x[4], lmerLength, \"read1_spark.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rdd_Graph.getNumPartitions()\n",
    "# evList.tolist()\n",
    "#%time \n",
    "# rdd_Graph.collect()\n",
    "# et_graph.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "operation above gets to setup_edges_device before erroring out. \n",
    "\n",
    "Based on console messages we're not calculating edges properly\n",
    "\n",
    "Maybe try calling functions one by one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#findEulerTour(evList, eeList, levEdgeList, entEdgeList, edgeCount, vertexCount, lmerLength, outfile)\n",
    "rdd_tour = rdd_Graph.mapPartitions(lambda x: ec.findEulerTour(x[1], x[0], x[2], x[3], x[5], x[4], lmerLength, \"ba_spark.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rdd_tour.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_constructDebruijnGraph_ functions:\n",
    "\n",
    "    vals = readLmersKmersCuda(readBuffer, readLength, readCount, lmerLength, h_lmerKeys, h_lmerValues,\n",
    "                              lmerCount, h_kmerKeys, h_kmerValues, kmerCount, numReads)\n",
    "    results = gh.create_hash_table_device(h_kmerKeys, h_kmerValues, kmerCount, d_TK, d_TV, tableLength,\n",
    "                                          d_bucketSize, bucketCount)\n",
    "\n",
    "    tableLength = results[0]\n",
    "    d_bucketSize = results[1]\n",
    "    bucketCount = results[2]\n",
    "    d_TK = results[3]       # kmer keys in sorted buckets\n",
    "    d_TV = results[4]       # kmer values in sorted buckets\n",
    "\n",
    "    d_ev, d_ee, d_levEdge, d_entEdge, kmerCount, edgeCount = \\\n",
    "        db.construct_debruijn_graph_device(h_lmerKeys, h_lmerValues, lmerCount, h_kmerKeys, kmerCount, lmerLength,\n",
    "        d_TK, d_TV, d_bucketSize, bucketCount, d_ev, d_levEdge, d_entEdge, d_ee, readLength, readCount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vals =  buffer.mapPartitions(lambda x: readLmersKmersCuda(x, readLength, readCount, lmerLength, h_lmerKeys, h_lmerValues,\n",
    "#                               lmerCount, h_kmerKeys, h_kmerValues, kmerCount, numReads))\n",
    "\n",
    "hail_mary = data.mapPartitions(lambda x: ec.assemble2(k, x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Spark 1.6.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
